---
title: "Cleaning and Processing with code pulled in"
author: "Cassia Roth"
date: "`r Sys.Date()`"
output: html_document
---

Please note I used ChatGPT and CoPilot throughout to help troubleshoot code. I'm not sure that I did this file and the `processingcode.R` files correctly. `processingfile-v1.qmd` is correct.

# Processing script

This contains the same code and comments/information as `processingcode.R`. This quarto file pulls in from the R script using the code chunk labels.

# Setup

This needs to run to load the R script so we can include the code chunks below

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('processingcode.R')
```


```{r, packages, message = FALSE, warning = FALSE}
```

# Data loading

Note that for functions that come from specific packages (instead of base R), you can specify both package and function: `package::function()`. Although not required, specifying the package makes it clearer where the function is coming from. This text is adapted from Dr. Andreas Handel.

```{r, loaddata}
```
From this initial read, we can see that the data have 2845 observations and 23 variables. The variables are characterized as character and numerical.

# Check data

First we can look at the Codebook and practice table making.
```{r, checkdata}
```

# Explore data

There are several ways of looking at the data. This code comes from Dr. Andreas Handel.

```{r, exploredata}
```

# Clean data

I created this dataset from published clinical notes from a specific maternity hospital in Rio de Janeiro, Brazil for multiple projects, mostly of a qualitative nature. Thus, I was interested in the notes clinicians included on difficult deliveries and their outcomes. These variables include qualitative information that can be excluded from our processed data, since we are not looking at qualitative notes on cases. We also don't need the citation information to run regressions. So we will be removing the following variables as we process our data: `VolN`, `Page`, `Number`, `Nationality_Notes`, `Birth_Notes`, `Maternal_Notes`, `Fetal_Notes`, `CauseofDeath`, `PreviousHistory`, and `Notes`.

```{r, cleandata}
```

Despite removing variables we won't use because of their qualitative nature, we still have many missing observations for some of the variables. Let's check for missing data.

We can see that `CivilStatus` has 2834 missing observations and `GestationalAge_Months` has 2789 missing observations. This is a lot. Since I am not interested in understanding the relationship between marital status and infant birth weight and since gestational age in months refers to spontaneous abortions that occurred in the clinic, we can also exclude both these variables, the latter because an abortion precludes the existence of a birth.

Now we can check for missing data again.

Now I will work on transforming character variables into factors.

# Create datasets for analysis

There are still missing observations for all the remaining variables except `Date`. For now, I will keep these observations intact because I want the maximum number of observations for summary statistics of all variables. Although the sample size for these summary statistics will then differ from that of my linear regression model, I believe more observations at the exploratory stage provide a fuller picture of what type of patient went to the hospital during the time period under study. Here, I will create two datasets: one for summary statistics and one for the linear regression model.

The latter (linear regression) will exclude the variables `MaternalOutcome` and `Birth`. 

```{r, createdatasets}
```
Now, I can see that my `ML_linear` dataset has no missing data. It has 2032 observations and 11 variables. This might change as I begin analysis. For example, I might want to exclude infant birth length, which is included at the moment.

# Save data 

Finally, I will save the clean data both the `ML_Linear` dataset and the `ML_summary` dataset as RDS files
.
```{r, savedata}
```

# Notes

This script is a work in progress. I will be adding more to it as I continue to work on the data. 
